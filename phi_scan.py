#!/usr/bin/env python
# coding: utf-8
# %%


import pandas as pd
import numpy as np
from tqdm import tqdm
# import matplotlib.pyplot as plt
import json
from xgboost import XGBClassifier
import datetime
import argparse


# %%


# from AutoDeidentifyNet import *

from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix,accuracy_score, roc_curve, auc, precision_recall_curve, average_precision_score
from sklearn.metrics import roc_auc_score, confusion_matrix,accuracy_score, roc_curve, auc, precision_recall_curve

from pandas.api.types import is_numeric_dtype


# %%


from sklearn.metrics import roc_curve, confusion_matrix , auc, precision_recall_curve, average_precision_score
from sklearn import metrics

def print_metrics(y_true, y_pred):
    false_positive_rate, recall, thresholds = roc_curve(y_true, y_pred)
    roc_auc = auc(false_positive_rate, recall)
    auprc = average_precision_score(y_true, y_pred)
    print('AUC: ',roc_auc, "AUPRC: ", auprc)


    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_pred)

    # calculate the g-mean for each threshold
    gmeans = np.sqrt(tpr * (1-fpr))
    ix = np.argmax(gmeans)
    print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))


#     tn, fp, fn, tp = confusion_matrix(y_true, y_pred > thresholds[ix]).ravel()
#     print('TN: ', tn, ", FP: ",fp, ", FN:", fn, ", TP:", tp)
#     print("==> Sensitivity (Recall, TPR): %.3f"%(tp/(tp+fn)))
#     print("==> Specifity: %.3f"%(tn/(tn+fp)))
#     print("==> Positive Predictive Value (PPV) (Precision): %.3f"%(tp / (tp + fp)))
#     print("==> Negative Predictive Value (NPV): %.3f"%(tn / (tn + fn)))
#     print("==> Accuracy: %.3f"%((tp+tn)/(tn+ fp+ fn+tp)))
#     print("==> F1 score: %.3f"%((2*tp)/(2*tp + fp + fn)))

#     ns_probs = [0 for _ in range(len(y_true))]
    
#     # calculate roc curves
#     ns_fpr, ns_tpr, _ = roc_curve(y_true, ns_probs)
#     lr_fpr, lr_tpr, _ = roc_curve(y_true, y_pred)
#     # plot the roc curve for the model
#     plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill, AUC = %0.2f' % 0.5)
#     plt.plot(lr_fpr, lr_tpr, marker='.', label = 'Our model: AUC = %0.2f' % roc_auc)
#     # axis labels
#     plt.xlabel('False Positive Rate')
#     plt.ylabel('True Positive Rate')
#     # show the legend
#     plt.legend()
#     # show the plot
#     plt.show()
    
#     lr_precision, lr_recall, _ = precision_recall_curve(y_true, y_pred)
#     lr_auc = auc(lr_recall, lr_precision)
#     # summarize scores
# #     print('Logistic: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))
#     # plot the precision-recall curves
#     y_true = np.array(y_true)
#     no_skill = len(y_true[y_true==1]) / len(y_true)
#     plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No skill, AUPRC = %0.2f' % no_skill)
#     plt.plot(lr_recall, lr_precision, marker='.',label = 'Our model: AUPRC = %0.2f' % auprc)
#     # axis labels
#     plt.xlabel('Recall')
#     plt.ylabel('Precision')
#     # show the legend
#     plt.legend(loc = 'upper right')
#     # show the plot
#     plt.show()

    y_pred_01 = y_pred.apply(lambda x: int(x >= thresholds[ix]))

    return y_pred_01


# %%
# ### flatten dict 
# https://www.freecodecamp.org/news/how-to-flatten-a-dictionary-in-python-in-4-different-ways/
def create_training_data(json_data):
    from collections.abc import MutableMapping

    def _flatten_dict_gen(d, parent_key, sep):
        for k, v in d.items():
            new_key = parent_key + sep + k if parent_key else k
            if isinstance(v, MutableMapping):
                yield from flatten_dict(v, new_key, sep=sep).items()
            else:
                yield new_key, v


    def flatten_dict(d: MutableMapping, parent_key: str = '', sep: str = '.'):
        return dict(_flatten_dict_gen(d, parent_key, sep))

    feature_list_1 = ['column_name', 'data_type', 'categorical', 'order'] 
    feature_list_2 = set()

    train_data_all = []

    for i in tqdm(range(len(json_data['data_stats']))):
        train_data = flatten_dict(json_data['data_stats'][i])
        train_data_all.append(train_data)


    # remove some json features
    feature_list = set([j for i in train_data_all for j in i.keys()])
    feature_list_2 = []
    for j in feature_list:
        if 'categorical_count' in j: continue
        if 'null_types_index' in j: continue
        if  'null_types' in j: continue
        if 'mode' in j : continue
        if 'samples' in j: continue
        if 'categories' in j : continue
        if 'bin_edges' in j : continue 
        if 'bin_counts' in j: continue
        if 'column_name' in j: continue
        if 'format' in j: continue
        if 'order' in j: continue
        if 'categorical' in j: continue
            
        feature_list_2.append(j)

    # build training dataframe
    df_train_data_all = pd.DataFrame(columns = feature_list_2)
    for data in train_data_all:
        df_data = pd.DataFrame(data, columns = feature_list_2, index=[data['column_name']])
        df_train_data_all = df_train_data_all.append(df_data)

    # create 0/1 mask data
    df_train_data_all_mask = df_train_data_all.notnull().astype('int')
    df_train_data_all_mask.columns = [i + str('_01') for i in df_train_data_all_mask.columns]

    df_train_data_all = df_train_data_all.join(df_train_data_all_mask)
    return df_train_data_all

# %%
# define 
HIPPA = ["Financial Number",
"1 MRNOrganization",
"2 MRNOrganization",
"3 MRNOrganization",
"4 MRNOrganization",
"Patient Name", "Full name", "Given name", "Surname", "First name", "Last name",
"Social Security Number", "SSN", "Social Security", "National Identifier Number", "Taxpayer Identification Number" ,"TIN"
"Medical Record Number", "MRN", "Medical ID, Record number", "Patient ID", "Health Record Number",
"Person Location- Facility (Admit)",
"Admit Date & Time",
"Discharge Date & Time",
"Person Address- Zip Code",
"Birth Date", "Birth year", "Year of birth"
"Age","Age in years"
"Ethnic Group",
"Ethnic Group.1",
"Race",
"Race.1",
# "Sex",
'subject_id',
'admittime',
'dischtime',
'deathtime',
'language',
'religion',
'marital_status',
'ethnicity',
'subject_id',
# 'gender',
'dod_hosp',
'dod_ssn',
'mimic_id',
'subject_id',
'hadm_id',
'icustay_id'
'intime',
'outtime',
'dob',
'dod',
]

# %%
fix_column_name = ['statistics.kurtosis', 'statistics.mean', 'statistics.histogram',
       'statistics.times.sum', 'statistics.null_count',
       'statistics.unique_count', 'statistics.data_type_representation.int',
       'statistics.quantiles.0', 'statistics.stddev', 'statistics.sample_size',
       'statistics.median_abs_deviation', 'statistics.times.kurtosis',
       'statistics.data_type_representation.float', 'statistics.unique_ratio',
       'statistics.sum', 'statistics.num_zeros', 'statistics.gini_impurity',
       'statistics.times.min', 'data_type', 'statistics.skewness',
       'statistics.times.variance', 'statistics.times.datetime',
       'statistics.quantiles.2', 'statistics.times.histogram_and_quantiles',
       'statistics.variance', 'statistics.quantiles.1',
       'statistics.times.num_zeros', 'statistics.unalikeability',
       'statistics.times.num_negatives',
       'statistics.data_type_representation.datetime', 'statistics.median',
       'statistics.min', 'statistics.num_negatives', 'statistics.max',
       'statistics.times.skewness', 'statistics.times.max',
       'statistics.kurtosis_01', 'statistics.mean_01',
       'statistics.histogram_01', 'statistics.times.sum_01',
       'statistics.null_count_01', 'statistics.unique_count_01',
       'statistics.data_type_representation.int_01',
       'statistics.quantiles.0_01', 'statistics.stddev_01',
       'statistics.sample_size_01', 'statistics.median_abs_deviation_01',
       'statistics.times.kurtosis_01',
       'statistics.data_type_representation.float_01',
       'statistics.unique_ratio_01', 'statistics.sum_01',
       'statistics.num_zeros_01', 'statistics.gini_impurity_01',
       'statistics.times.min_01', 'data_type_01', 'statistics.skewness_01',
       'statistics.times.variance_01', 'statistics.times.datetime_01',
       'statistics.quantiles.2_01',
       'statistics.times.histogram_and_quantiles_01', 'statistics.variance_01',
       'statistics.quantiles.1_01', 'statistics.times.num_zeros_01',
       'statistics.unalikeability_01', 'statistics.times.num_negatives_01',
       'statistics.data_type_representation.datetime_01',
       'statistics.median_01', 'statistics.min_01',
       'statistics.num_negatives_01', 'statistics.max_01',
       'statistics.times.skewness_01', 'statistics.times.max_01',
       'statistics.precision.var', 'statistics.precision.max',
       'statistics.times.precision', 'statistics.precision.confidence_level',
       'statistics.precision.min', 'statistics.precision.margin_of_error',
       'statistics.precision.sample_size', 'statistics.precision.mean',
       'statistics.precision.std', 'statistics.precision.var_01',
       'statistics.precision.max_01', 'statistics.times.precision_01',
       'statistics.precision.confidence_level_01',
       'statistics.precision.min_01',
       'statistics.precision.margin_of_error_01',
       'statistics.precision.sample_size_01', 'statistics.precision.mean_01',
       'statistics.precision.std_01']


# %%
def main(model, df, df_json):
    X = create_training_data(df_json)
    
    ## Columns name needs to be fixed
    # add these fix_column_name
    X = pd.concat([pd.DataFrame(columns = fix_column_name), X]) 
    
    # X_join could possibily contain unseen columns -> delete
    X = X[fix_column_name]
    assert X.shape[1] == 90



    # create HIPPA label
    X['HIPPA'] = 0
    X[X.index.isin(HIPPA)] = 1
    
    # change object to float
    for i in X.columns:
        X[i] = pd.to_numeric(X[i],errors='coerce')
    
    
    # sampe 5000 to form text
    data_1_5000 = df.loc[:5000, :]

    data_1_5000 = data_1_5000.replace('Unknown', np.nan) # unknown -> nan
    data_1_5000 = data_1_5000.replace('Other', np.nan)  # other -> nan

#     # nan to random choice
#     import random 
#     for c in data_1_5000.columns:
#         l = list(set(data_1_5000[c][data_1_5000[c].notna()].tolist()))
#     #     print(l)
#         if not l:
#             l = [0]
#         data_1_5000[c].fillna(random.choice(l), inplace=True)

#     y_pred_2 = {}

#     whitelist_file = 'whitelist.pkl'

#     start_time_all = time.time()
#     with open(whitelist_file, "rb") as fin:
#         whitelist = pickle.load(fin)

#     for c in data_1_5000.columns:
#         print(c)
#         text = c + ' , '
#         for r in data_1_5000[c].tolist()[:100]:
#             text += str(r) + ' , '
#         try:
#             p = filter_task(text, whitelist)
#         except: 
#             print(c, ' is not english')
#             p = 0
#         print("{:.2f}%".format(p*100))
#         y_pred_2[c] = p

    print(X.shape)
    df_pred_result = pd.DataFrame({'ML prediction result': model.predict_proba(X.drop(columns=['HIPPA']))[:, 1]}).set_index([X.index])
#     df_pred_result['REgular expression result'] = pd.DataFrame.from_dict(y_pred_2, orient='index').set_index([X.index])
    df_pred_result = df_pred_result.join(X[['HIPPA']])

    predict_01 = print_metrics(df_pred_result['HIPPA'], df_pred_result['ML prediction result'])

    df_pred_result['ML prediction result 0/1'] = predict_01
    
    df_pred_result = df_pred_result[['HIPPA', 'ML prediction result', 'ML prediction result 0/1']]
    return df_pred_result

# %%

def phi_scan(original_data_path,json_file_path,model_path,output_path):
    
    # load json data
    with open(json_file_path) as f:
        json_data = json.load(f)
    print('Finish loading data 1 json')


    df_data = pd.read_csv(original_data_path)
    print('Finish loading data 1')

    
    # load model
    model_xgb = XGBClassifier()
    model_xgb.load_model(model_path)
    
    csv_result = main(model_xgb, df_data, json_data)
    
    # csv_result.to_csv('Prediction_result_' + str(datetime.datetime.now()) +'.csv')
    
    # csv_result.to_csv('{}_prediction_result.csv'.format(args.json_file_path.split('profile')[0]))
    csv_result.to_csv(output_path)
    

# %%
